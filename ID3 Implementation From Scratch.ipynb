{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the names of the feature sin the dataset\n",
    "feature_names = ['stream', 'slope', 'elevation', 'vegetation']\n",
    "\n",
    "# dictionary object that lists for each feature (key) the set of levels in the domain of the feature (value)\n",
    "feature_levels = {'stream':['false','true'], \n",
    "                 'slope':['flat','moderate','steep'], \n",
    "                 'elevation':['low','medium','high','highest'], \n",
    "                 'vegetation':['chaparral','riparian','conifer']}\n",
    "\n",
    "\n",
    "\n",
    "# Then we create our dataset of training instances\n",
    "# the first row in the dataset (dataset[0]) lists the names of the features\n",
    "\n",
    "dataset=[['false','steep','high','chaparral'],\n",
    "          ['true','moderate','low','riparian'],\n",
    "          ['true','steep','medium','riparian'],\n",
    "          ['false','steep','medium','chaparral'],\n",
    "          ['false','flat','high','conifer'],\n",
    "          ['true','steep','highest','conifer'],\n",
    "          ['true','steep','high','chaparral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target feature: ['vegetation']\n",
      "Instance 1 Target Feature Value = ['chaparral']\n",
      "Instance 2 Target Feature Value = ['riparian']\n",
      "Instance 3 Target Feature Value = ['riparian']\n",
      "Instance 4 Target Feature Value = ['chaparral']\n",
      "Instance 5 Target Feature Value = ['conifer']\n",
      "Instance 6 Target Feature Value = ['conifer']\n",
      "Instance 7 Target Feature Value = ['chaparral']\n",
      "Instance 3 = ['true', 'steep', 'medium', 'riparian']\n",
      "The elevation feature for instance 5 in the datset is = high\n"
     ]
    }
   ],
   "source": [
    "# 1. The name of the target feature\n",
    "# The target feature name is listed in the last element of feature_names\n",
    "print(\"Target feature: \" + str(feature_names[-1:]))\n",
    "\n",
    "# 2. List the target feature values for the instances in the dataset\n",
    "for i in range(0, len(dataset)):\n",
    "    print(\"Instance \" + str(i+1) + \" Target Feature Value = \" + str(dataset[i][-1:]))\n",
    "\n",
    "# 3. The description of the 3rd instance in the dataset\n",
    "# Remember, Python lists are zero indexed.  \n",
    "print(\"Instance 3 = \" + str(dataset[2]))\n",
    "\n",
    "\n",
    "# 4. The value of the 'elevation' feature for the 5th instance in the dataset\n",
    "index = feature_names.index('elevation')\n",
    "print(\"The elevation feature for instance 5 in the datset is = \" + str(dataset[4][index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes a list of values as input and return the entropy of the list\n",
    "    \n",
    "def entropy(values):\n",
    "    \n",
    "   from math import log\n",
    "   val_counts={}\n",
    "   for v in values:\n",
    "        if v in val_counts.keys():\n",
    "            count = val_counts[v]\n",
    "            val_counts[v]=count+1\n",
    "        else:\n",
    "            val_counts[v]=1\n",
    "   entropy=0.0\n",
    "   for v in val_counts.keys():\n",
    "      p=float(val_counts[v])/len(values)\n",
    "      entropy=entropy+(p*log(p,2))\n",
    "   if entropy!=0:\n",
    "        entropy=entropy*-1\n",
    "   return (entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy animalset1 = 0.4394969869215134\n",
      "Entropy animalset2 = 1.0\n",
      "Entropy animalset3 = 2.321928094887362\n"
     ]
    }
   ],
   "source": [
    "animalset1 = ['cat', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse']\n",
    "print(\"Entropy animalset1 = \" + str(entropy(animalset1)))\n",
    "\n",
    "animalset2 = ['horse', 'horse', 'horse', 'horse', 'horse', 'dog', 'dog', 'dog', 'dog', 'dog']\n",
    "print(\"Entropy animalset2 = \" + str(entropy(animalset2)))\n",
    "\n",
    "animalset3 = ['horse', 'horse', 'dog', 'dog', 'cat', 'cat', 'sheep', 'sheep', 'lion', 'lion']\n",
    "print(\"Entropy animalset3 = \" + str(entropy(animalset3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function extracts a column of values from a table and returns it as a list\n",
    "\n",
    "def get_feature_column(featureindex=-1, dataset=None):\n",
    "    featureColumn = []\n",
    "    for i in range(0,len(dataset)):\n",
    "        featureColumn.append(dataset[i][featureindex])\n",
    "    return(featureColumn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column = ['chaparral', 'riparian', 'riparian', 'chaparral', 'conifer', 'conifer', 'chaparral']\n",
      "Entropy of dataset1: 1.5566567074628228\n"
     ]
    }
   ],
   "source": [
    "# Let's test it.\n",
    "# We assume that the target feature is the rightmost column in the dataset\n",
    "# so to get the index of this feature we simply subtract 1 from the length of the first row in the dataset. \n",
    "\n",
    "target_index = len(dataset[0])-1\n",
    "\n",
    "\n",
    "# We can then use the get_feature_column function to extract the feature column into a list\n",
    "\n",
    "target_column = get_feature_column(target_index,dataset)\n",
    "print(\"Target column = \" + str(target_column))\n",
    "print(\"Entropy of dataset1: \" + str(entropy(target_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the function \n",
    "def create_partitions(featureIndex=-1,dataset=None):\n",
    "    partitions={}\n",
    "    for i in range(0, len(dataset)):\n",
    "        tmpValue = dataset[i][featureIndex]\n",
    "        tmpList = []\n",
    "        if tmpValue in partitions.keys():\n",
    "            tmpList = partitions[tmpValue]\n",
    "            tmpList.append(dataset[i])\n",
    "        else:\n",
    "            tmpList.append(dataset[i])\n",
    "        partitions[tmpValue]=tmpList\n",
    "    return(partitions)\n",
    "    \n",
    "    \n",
    "def calculate_remainder(partitions):\n",
    "    remainder=0\n",
    "    #we assume that the target feature is the rightmost column in an instance\n",
    "    #so we can get the target index by retrieveing an one of the value from the\n",
    "    #partitions datasture (next(iter(partitions.values()))) and subtracting 1\n",
    "    #from the length of this instance.\n",
    "    exampleInstance=(next(iter(partitions.values())))[0]\n",
    "    targetIndex=len(exampleInstance)-1\n",
    "    #in order to be able to weight the entropy of each partition\n",
    "    #we need to know the total number of examples across all the partitions\n",
    "    #this number defines the denominator in the weight term\n",
    "    #we store this number in the variable size_dataset\n",
    "    size_dataset =0\n",
    "    for k in partitions.keys():\n",
    "        size_dataset = size_dataset + len(partitions[k])\n",
    "    #we are no ready to calculate the remaining entropy by calculating a\n",
    "    #the weighted sum of the entropy for each partition\n",
    "    for k in partitions.keys():\n",
    "        #calculate the entropy for each partition\n",
    "        tmpPartition = partitions[k]\n",
    "        targetColumn = get_feature_column(targetIndex,tmpPartition)\n",
    "        ent = entropy(targetColumn)\n",
    "        #calculate the weight for each partition\n",
    "        weight = len(tmpPartition)/size_dataset\n",
    "        #sum the weighting remaining entropy for each partition\n",
    "        remainder = remainder + (weight * ent)\n",
    "    return(remainder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paritions created:\n",
      "---------------------\n",
      "steep:\n",
      "\t\t['false', 'steep', 'high', 'chaparral']\n",
      "\t\t['true', 'steep', 'medium', 'riparian']\n",
      "\t\t['false', 'steep', 'medium', 'chaparral']\n",
      "\t\t['true', 'steep', 'highest', 'conifer']\n",
      "\t\t['true', 'steep', 'high', 'chaparral']\n",
      "moderate:\n",
      "\t\t['true', 'moderate', 'low', 'riparian']\n",
      "flat:\n",
      "\t\t['false', 'flat', 'high', 'conifer']\n",
      "---------------------\n",
      "Remaining entropy after partitioning: 0.9792504246104776\n"
     ]
    }
   ],
   "source": [
    "#use the create_paritions function to split the dataset using descriptive feature 'slope' (feature index = 1)\n",
    "slope_partitions = create_partitions(1,dataset)\n",
    "print(\"Paritions created:\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "for k in slope_partitions.keys():\n",
    "    print(str(k)+ \":\")\n",
    "    instances = slope_partitions[k]\n",
    "    for inst in instances:\n",
    "        print('\\t\\t' +str(inst))\n",
    "\n",
    "print(\"---------------------\")\n",
    "\n",
    "    \n",
    "#use the calculate_remainder function to calculate the entropy remaining after we split dataset using the slop feature\n",
    "rem = calculate_remainder(slope_partitions)\n",
    "print(\"Remaining entropy after partitioning: \" + str(rem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(feature_index=-1, dataset=[]):\n",
    "\n",
    "    #calculate the entropy of the dataset before we partition it using the feature \n",
    "\n",
    "    target_index = len(dataset[0])-1\n",
    "    target_column = get_feature_column(target_index,dataset)\n",
    "    \n",
    "    h = entropy(target_column)\n",
    "\n",
    "    #calculate the remaining entropy after we partition the dataset using the feature\n",
    "    partitions = create_partitions(feature_index,dataset)\n",
    "\n",
    "    rem = calculate_remainder(partitions)\n",
    "\n",
    "    #calculate the information gain for the feature\n",
    "    ig = h - rem\n",
    " \n",
    "    return(ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information gain for feature stream is: 0.30595849286804166\n",
      "The information gain for feature slope is: 0.5774062828523452\n",
      "The information gain for feature elevation is: 0.8773870642966131\n"
     ]
    }
   ],
   "source": [
    "# Iterate across the indexes of the descriptive features in the dataset\n",
    "# (we assume that the target feature is the last feature in the dataset)\n",
    "# and call the information_gain function for each descriptive feature by passing in the feature index\n",
    "\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "\n",
    "    ig = information_gain(i,dataset)\n",
    "\n",
    "    print(\"The information gain for feature \" + feature_names[i] + \" is: \" + str(ig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tree_node:\n",
    "  def __init__(self, feature_name='', feature_index='', branches={}, instances=[], prediction=''):\n",
    "\n",
    "    self.feature_name=feature_name    #stores the name of the feature tested at this node\n",
    "    self.feature_index=feature_index  #stores the index of the feature column in the dataset\n",
    "    self.branches=branches          #a dictionary object: each key=level of test feature, each value=child node of this node\n",
    "    self.instances=instances        #in a leaf node this list stores the set of instances that ended up at the leaf\n",
    "    self.prediction=prediction      #in a leaf node this variable stores the target level returned as a prediction by the node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return true if all the instances in the dataset D have the same target level\n",
    "# A handy way to check for this condition is by checking if the entropy of the \n",
    "# aataset with respect to the target feature == 0\n",
    "\n",
    "def all_same(D=[]):\n",
    "\n",
    "    if len(D) > 0:\n",
    "        target_index = len(D[0])-1\n",
    "        target_column = get_feature_column(target_index,D)\n",
    "\n",
    "        if entropy(target_column) == 0:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "    \n",
    "\n",
    "#return the majority target level in the instances list\n",
    "def majority_target_level(D):\n",
    "\n",
    "        #assume the target feature is the last feature in each instance\n",
    "        target_index = len(D[0])-1\n",
    "\n",
    "        #extract the set of target levels in the instances at this node\n",
    "        target_column = get_feature_column(target_index,D)\n",
    "\n",
    "        #create a dictionary object that records the count for each target level\n",
    "        levels_count = {}\n",
    "\n",
    "        for l in target_column:\n",
    "            if l in levels_count.keys():\n",
    "                levels_count[l]+=1\n",
    "            else:\n",
    "                levels_count[l]=1\n",
    "\n",
    "        #find the target level with the max count\n",
    "        #for ease of implementation we break ties in max counts\n",
    "        #by symply returning the first level we find with the max count\n",
    "\n",
    "        max_count = -999999\n",
    "        majority_level = ''\n",
    "\n",
    "        for k in levels_count.keys():\n",
    "            if levels_count[k] > max_count:\n",
    "                max_count=levels_count[k]\n",
    "                majority_level=k\n",
    "\n",
    "        return(majority_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This **ID3** implementation takes 5 parameters. \n",
    "# The 5 parameters are as follows:\n",
    "#1. d = list of descriptive features not yet used on the path from the root to the current node \n",
    "#2. D = the set of training instances that have descended the path to this node\n",
    "#3. parentD = the set of training instances at the parent of this node\n",
    "#4. feature_levels = a dictionary object that lists for each feature (key) the set of levels in the domain of the feature (value)\n",
    "#5. feature_names = a list of the names of the features in the dataset\n",
    "\n",
    "def id3(d=[], D=[], parentD=[], feature_levels={}, feature_names=[]):\n",
    "\n",
    "    if all_same(D):\n",
    "\n",
    "        return tree_node(feature_name='',feature_index=-1,branches={},instances=D,prediction=D[0][len(D[0])-1])\n",
    "\n",
    "    elif len(d) == 0:\n",
    "\n",
    "        return tree_node(feature_name='',feature_index=-1,branches={},instances=D,prediction=majority_target_level(D))\n",
    "\n",
    "    elif len(D) == 0:\n",
    "\n",
    "        return tree_node(feature_name='',feature_index=-1,branches={},instances=D,prediction=majority_target_level(parentD))\n",
    "\n",
    "    else:\n",
    "\n",
    "        d_best = \"\"\n",
    "        best_index = -1\n",
    "        max_IG = -9999\n",
    "\n",
    "        for f in d:\n",
    "\n",
    "            feature_index = feature_names.index(f)\n",
    "            tmp_IG = information_gain(feature_index,D)\n",
    "\n",
    "            if tmp_IG > max_IG:\n",
    "                max_IG = tmp_IG\n",
    "                d_best = f\n",
    "                best_index=feature_index\n",
    "\n",
    "        node = tree_node(feature_name=d_best,feature_index=best_index,branches={},instances=[],prediction='')\n",
    "\n",
    "        #partition the dataset using the feature with the highest information gain\n",
    "        partitions=create_partitions(best_index,D)\n",
    "\n",
    "        #remove d_best from the list of features passed down to the children of this node\n",
    "        d_new = [ f for f in d if not f.startswith(d_best) ]\n",
    "\n",
    "        #iterate across all the levels of the feature and create a branch for each level\n",
    "        #we use arg4 for this because it may be that one or more of the levels of the feature do not appears in D\n",
    "\n",
    "        for level in feature_levels[d_best]:\n",
    "\n",
    "            if level in partitions.keys():\n",
    "                d_new_1 = partitions[level]\n",
    "\n",
    "            else:\n",
    "\n",
    "                #if there is a feature level that does not occur in D\n",
    "                #then create a child node where the set of training instances\n",
    "                #at the node is empty\n",
    "\n",
    "                d_new_1 = []\n",
    "\n",
    "            node.branches[level]=id3(d_new,d_new_1,D,feature_levels,feature_names)\n",
    "\n",
    "        return(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featureNames[:-1] is the list of features names in the dataset apart from the last feature (i.e., excluding the target feature)\n",
    "\n",
    "tree = id3(feature_names[:-1], dataset, dataset, feature_levels, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--elevation:low\n",
      "   prediction = riparian\n",
      "--elevation:medium\n",
      "----stream:false\n",
      "     prediction = chaparral\n",
      "----stream:true\n",
      "     prediction = riparian\n",
      "--elevation:high\n",
      "----slope:flat\n",
      "     prediction = conifer\n",
      "----slope:moderate\n",
      "     prediction = chaparral\n",
      "----slope:steep\n",
      "     prediction = chaparral\n",
      "--elevation:highest\n",
      "   prediction = conifer\n"
     ]
    }
   ],
   "source": [
    "#This function prints out the tree in text format\n",
    "\n",
    "def print_tree(tree, indent='-'):\n",
    "\n",
    "    if tree.prediction == '':\n",
    "\n",
    "        indent+=\"--\"\n",
    "        for level in tree.branches.keys():\n",
    "            print(indent+tree.feature_name + ':' + str(level))\n",
    "            print_tree(tree.branches[level],indent)\n",
    "    else:\n",
    "\n",
    "        s = ''\n",
    "        for c in indent:\n",
    "            s+=' '\n",
    "        print(s+\" prediction = \" + tree.prediction)\n",
    "\n",
    "        \n",
    "#Here we call the function and pass in the tree we want to output\n",
    "print_tree(tree,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns a prediction from a tree for a query instance\n",
    "def make_prediction(query, tree, feature_levels):\n",
    "\n",
    "    if tree.prediction != '':\n",
    "\n",
    "        #if we have reached a leaf node return the prediction\n",
    "        return tree.prediction\n",
    "\n",
    "    else:\n",
    "\n",
    "        #otherwise descend the tree.\n",
    "        #1. get the level of the query instance for the node test feature\n",
    "\n",
    "        level = query[tree.feature_index]\n",
    "        for l in feature_levels[tree.feature_name]:\n",
    "\n",
    "            if l.startswith(level):\n",
    "                #2. find the branch that matchs this level and desencd the branch\n",
    "                return make_prediction(query,tree.branches[level],feature_levels)\n",
    "\n",
    "        print(\"No prediction!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ['true', 'moderate', 'low', '?'] Prediction: riparian\n",
      "Query: ['true', 'moderate', 'medium', '?'] Prediction: riparian\n",
      "Query: ['true', 'moderate', 'highest', '?'] Prediction: conifer\n",
      "Query: ['true', 'moderate', 'high', '?'] Prediction: chaparral\n",
      "Query: ['true', 'steep', 'high', '?'] Prediction: chaparral\n",
      "Query: ['true', 'flat', 'high', '?'] Prediction: conifer\n"
     ]
    }
   ],
   "source": [
    "query1 = ['true','moderate','low','?']\n",
    "print(\"Query: \" + str(query1) + \" Prediction: \" + make_prediction(query1, tree, feature_levels))\n",
    "\n",
    "query2 = ['true','moderate','medium','?']\n",
    "print(\"Query: \" + str(query2) + \" Prediction: \" + make_prediction(query2, tree, feature_levels))\n",
    "\n",
    "query3 = ['true','moderate','highest','?']\n",
    "print(\"Query: \" + str(query3) + \" Prediction: \" + make_prediction(query3, tree, feature_levels))\n",
    "\n",
    "query4 = ['true','moderate','high','?']\n",
    "print(\"Query: \" + str(query4) + \" Prediction: \" + make_prediction(query4, tree, feature_levels))\n",
    "\n",
    "query5 = ['true','steep','high','?']\n",
    "print(\"Query: \" + str(query5) + \" Prediction: \" + make_prediction(query5, tree, feature_levels))\n",
    "\n",
    "query6 = ['true','flat','high','?']\n",
    "print(\"Query: \" + str(query6) + \" Prediction: \" + make_prediction(query6, tree, feature_levels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
